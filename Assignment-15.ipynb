{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS-->Supervised - In Supervised learning the input data is labeled and the main task of the model is to either map the output with the input labels or when the input label is mapped as a continuous output.\n",
    "\n",
    "Semi-supervised - Semi-supervised learning falls in-between supervised and unsupervised learning. Here, while training the model, the training dataset comprises of a small amount of labeled data and a large amount of unlabeled data.\n",
    "\n",
    "Unsupervised - In unsupervised learning, the data is unlabeled and its goal is to find out the natural patterns present within data points in the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> 1 - Email Spam\n",
    "2 - Handwritten Digit Recognition\n",
    "3 - Image segmentation\n",
    "4 - Speech Recognition\n",
    "5 - DNA Expression Microarray\n",
    "6 - DNA Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> There are different phase of classification \n",
    "(1) data acquisition and segmentation\n",
    "(2) data preprocessing\n",
    "(3) feature extraction/dimension reduction\n",
    "(4) recognition and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Support Vector Machine(SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points.\n",
    "Various steps to be followed in SVM - \n",
    "1.Create a new classifier\n",
    "2.Select how you want to classify your data\n",
    "3.Import your training data\n",
    "4.Define the tags for your SVM classifier\n",
    "5.Tag data to train your classifier\n",
    "6.Set your algorithm to SVM\n",
    "7.Test Your Classifier\n",
    "8.Integrate the topic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Advantages of Support Vector Machine:\n",
    "1. SVM works relatively well when there is a clear margin of separation between classes.\n",
    "2. SVM is more effective in high dimensional spaces.\n",
    "3. SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "4. SVM is relatively memory efficient\n",
    "Disadvantages of Support Vector Machine:\n",
    "1. SVM algorithm is not suitable for large data sets.\n",
    "2. SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "3. In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform.\n",
    "4. As the support vector classifier works by putting data points, above and below the classifying hyperplane there is no probabilistic explanation for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n",
    "Most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity\n",
    "The KNN Algorithm\n",
    "1. Load the data\n",
    "2. Initialize K to your chosen number of neighbors\n",
    "3. For each example in the data\n",
    "\n",
    "4. Calculate the distance between the query example and the current example from the data.\n",
    "\n",
    "5. Add the distance and the index of the example to an ordered collection\n",
    "\n",
    "6. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
    "\n",
    "7. Pick the first K entries from the sorted collection\n",
    "\n",
    "8. Get the labels of the selected K entries\n",
    "\n",
    "9. If regression, return the mean of the K labels\n",
    "\n",
    "10. If classification, return the mode of the K labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Discuss the kNN algorithm's error rate and validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Validation error -\n",
    "By observing validation error rate we can interpret that At K=1, we were over fitting the boundaries. In Validation graph Error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. This value of K where error reaches minima should be used for all predictions\n",
    "Error rate -\n",
    "Error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, we can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K.Basically it helps us in finding the optimum value of k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> 1 - Train and test on the entire dataset. Train the model on the entire dataset. Test the model on the same dataset, and evaluate how well we did by comparing the predicted response values with the true response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> \n",
    "1. The k-nearest neighbor algorithm is imported from the scikit-learn package.\n",
    "2. Create feature and target variables.\n",
    "3. Split data into training and test data.\n",
    "4. Generate a k-NN model using neighbors value.\n",
    "5. Train or fit the data into the model.\n",
    "6. Predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A decision tree typically starts with a single node, which branches into possible outcomes. Each of those outcomes leads to additional nodes, which branch off into other possibilities. This gives it a treelike shape. There are three different types of nodes: chance nodes, decision nodes, and end nodes.\n",
    " A chance node, represented by a circle, shows the probabilities of certain results. \n",
    " A decision node, represented by a square, shows a decision to be made.\n",
    " An End node shows the final outcome of a decision path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Describe the different ways to scan a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> We can scan Decision Tree, by using parameters of 'Entropy' and 'Information Gain'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. Describe in depth the decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving regression and classification problems too.\n",
    "\n",
    "The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).\n",
    "\n",
    "In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the recordâ€™s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n",
    "\n",
    "Types of Decision Trees\n",
    "Types of decision trees are based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "Categorical Variable Decision Tree: Decision Tree which has a categorical target variable then it called a Categorical variable decision tree.\n",
    "Continuous Variable Decision Tree: Decision Tree has a continuous target variable then it is called Continuous Variable Decision Tree.\n",
    "\n",
    "1.Start with dataset.\n",
    "2.Add chance and decision nodes.\n",
    "3.Expand until you reach end points.\n",
    "4.Calculate tree values. \n",
    "5.Evaluate outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q13. In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> In the case of decision trees, the depth of the tress is the inductive bias. If the depth of the tree is too low, then there is too much generalisation in the model.\n",
    "Two approaches to avoiding overfitting are distinguished: pre-pruning and post-pruning . Results are given for pre-pruning using either a size or a maximum depth cutoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14.Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Advantages of Decision Trees - Interpretability, Less Data Preparation, Non-Parametric, Versatility, Non-Linearity.\n",
    "Disadvantages of Decision Tree- Overfitting , Feature Reduction & Data Resampling , Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Decision tree learning is generally best suited to problems with the following characteristics: Instances are represented by attribute-value pairs. There is a finite list of attributes and each instance stores a value for that attribute.\n",
    "\n",
    "Decision trees are extremely useful for data analytics and machine learning because they break down complex data into more manageable parts. They're often used in these fields for prediction analysis, data classification, and regression.\n",
    "\n",
    "Decision trees are used for handling non-linear data sets effectively.\n",
    "Decision Trees are most suitable for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16. Describe in depth the random forest model. What distinguishes a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Random Forests is a Machine Learning algorithm that tackles one of the biggest problems with Decision Trees: variance. Even though Decision Trees is simple and flexible, it is greedy algorithm. It focuses on optimizing for the node split at hand, rather than taking into account how that split impacts the entire tree.\n",
    "\n",
    "The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->out-of-bag error is an estimate of the error rate (which is 1 - accuracy) that this training approach has for new data from the same distribution. This estimate is based on the predictions that you get for each data point by using only averaging those trees, for which the record was not in the training data.\n",
    "It is the average error for each calculated using predictions from the trees that do not contain in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
